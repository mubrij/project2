# -*- coding: utf-8 -*-
"""Financial Inclusion in Africa

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pAw57aGWj_8G36kxZcniRmr8vEzf2KlK
"""

import requests
from tqdm.auto import tqdm

data_url = "https://api.zindi.africa/v1/competitions/fraud-detection-in-electricity-and-gas-consumption-challenge/files/train.zip"

token = {'auth_token': 'b'}
file_name = "train.zip"

def zindi_data_downloader(url, token, file_name):
    # Get the competition data
    competition_data = requests.post(url = data_url, data= token, stream=True)
    
    # Progress bar monitor download
    pbar = tqdm(desc=file_name, total=int(competition_data.headers.get('content-length', 0)), unit='B', unit_scale=True, unit_divisor=512)
    # Create and Write the data to colab drive in chunks
    handle = open(file_name, "wb")
    for chunk in competition_data.iter_content(chunk_size=512): # Download the data in chunks
        if chunk: # filter out keep-alive new chunks
                handle.write(chunk)
        pbar.update(len(chunk))
    handle.close()
    pbar.close()

zindi_data_downloader(url=data_url, token=token, file_name=file_name)

import numpy as np
import pandas as pd
from sklearn.preprocessing import scale

df = pd.read_csv("/content/Train (1).csv")
df.head()

x_data = df["bank_account"]

train_data = pd.read_csv('/content/Train (1).csv')

# Drop the unique identifier columns
train_data.drop(['uniqueid', 'bank_account'], axis=1, inplace=True)

# Convert the string variables to categorical variables
train_data['country'] = train_data['country'].astype('category')
train_data['year'] = train_data['year'].astype('category')
train_data['cellphone_access'] = train_data['cellphone_access'].astype('category')
train_data['gender_of_respondent'] = train_data['gender_of_respondent'].astype('category')
train_data['relationship_with_head'] = train_data['relationship_with_head'].astype('category')
train_data['marital_status'] = train_data['marital_status'].astype('category')
train_data['education_level'] = train_data['education_level'].astype('category')
train_data['job_type'] = train_data['job_type'].astype('category')


train_data.replace('Unknown', np.nan, inplace=True)

train_data['household_size'].fillna(train_data['household_size'].median(), inplace=True)
train_data['age_of_respondent'].fillna(train_data['age_of_respondent'].median(), inplace=True)

train_data['log_household_size'] = np.log(train_data['household_size'])
train_data = pd.get_dummies(train_data, drop_first=True)

numerical_vars = ['age_of_respondent', 'log_household_size']
train_data[numerical_vars] = (train_data[numerical_vars] - train_data[numerical_vars].mean()) / train_data[numerical_vars].std()

test_data = pd.read_csv('/content/Test.csv')

test_id = test_data['uniqueid']

test_data.drop('uniqueid', axis=1, inplace=True)

test_data['country'] = test_data['country'].astype('category')
test_data['year'] = test_data['year'].astype('category')
test_data['cellphone_access'] = test_data['cellphone_access'].astype('category')
test_data['gender_of_respondent'] = test_data['gender_of_respondent'].astype('category')
test_data['relationship_with_head'] = test_data['relationship_with_head'].astype('category')
test_data['marital_status'] = test_data['marital_status'].astype('category')
test_data['education_level'] = test_data['education_level'].astype('category')
test_data['job_type'] = test_data['job_type'].astype('category')

test_data.replace('Unknown', np.nan, inplace=True)

test_data['household_size'].fillna(test_data['household_size'].median(), inplace=True)
test_data['age_of_respondent'].fillna(test_data['age_of_respondent'].median(), inplace=True)

test_data['log_household_size'] = np.log(test_data['household_size'])

test_data = pd.get_dummies(test_data, drop_first=True)

numerical_vars = ['age_of_respondent', 'log_household_size']
test_data[numerical_vars] = (test_data[numerical_vars] - test_data[numerical_vars].mean()) / test_data[numerical_vars].std()

test_df = pd.DataFrame(scale(test_data))
test_df.head()

df = pd.DataFrame(scale(train_data))

df.head()



y_df = x_data.map({'Yes': 1, 'No': 0})
y_df

import tensorflow as tf
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense,Dropout

X_train, X_val, y_train, y_val = train_test_split(df, y_df, test_size=0.2, random_state=42)

model = Sequential()
model.add(Dense(120, input_shape=(34,), activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(120, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(120, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(120, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

history = model.fit(X_train, y_train, batch_size=32, epochs=1000, validation_data=(X_val, y_val))

# Evaluate the model on the validation set
loss, accuracy = model.evaluate(X_val, y_val)
print('Validation loss:', loss)
print('Validation accuracy:', accuracy)

from tensorflow.keras.models import load_model

model = load_model('model.h5')

predictions = model.predict(test_data)

# Map the predictions to binary values
predictions = np.round(predictions).astype(int)

# Convert the predictions to a DataFrame with the required format for submission
submission_df = pd.DataFrame({'uniqueid': test_data['uniqueid'] + ' x ' + test_data['country'], 'bank_account': predictions.reshape(-1)})
submission_df.to_csv('Financial_Inclusion_in Africa_submission.csv', index=False)